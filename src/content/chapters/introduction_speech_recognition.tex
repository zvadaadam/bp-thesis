\chapter{Speech Recognition}

Speech recognition is the task of converting speech audio to text representation.
It has been attracting researchers for many years with a goal to produce efficient speech recognizer, because it's a very easy and natural human-machine interface tool.

Speech recognition system takes audio signal as an input and predicts the text transcript.
Arbitrary speech recognizers are normally divided into two important building blocks as shown on \ref{pic:speech_recognizer.png}.
The Feature Extractor block generates a sequence of feature vectors which are then fed to the recognizer block generating the correct output words.

\imagefigurelarge{speech_recognizer.png}{Basic building blocks of a Speech Recognizer \cite{recognizer_blocks}}
%http://www.informatik.uni-ulm.de/ni/staff/FSchwenker/lit/papers/zegers98speech.pdf

\section{Feature Extraction}

The feature extraction (FE) block used in speech recognition should aim towards reducing the complexity of the problem,
it should derive descriptive features from speech signal to enable a classification of sounds.
It is needed because the raw speech signal contains other information besides the linguistic message which would be counterproductive for recognizer.



\subsection{Preprocessing}

It is advantageous to apply preprocessing to raw speech signal before moving to feature extraction block.
Using some type of preprocessing leads to easier feature extraction and faster training phase.

Advantageous preprocessing method is to downsample given speech signal.
Speech is mostly recorded with a sampling frequency of 44.1kHz or 48kHz, although speech signal has frequency components in the audio frequency from 20Hz to 20kHz\cite{range_speech}.
%http://iitg.vlab.co.in/?sub=59&brch=164&sim=474&cnt=1
That's because of Nyquist-Shannon sampling Theorem \cite{shannon}, a time-continuous signal that is band-limited to a certain finite frequency needs to be sampled with a double the maximum frequency.
Since human speech has a relatively low bandwidth, mostly up to 8kHz.
That means that sampling frequency of 16KHz is sufficient for speech recognition tasks\cite{speech_preprocessing}.
%http://recognize-speech.com/preprocessing

Other part of preprocessing is to remove the parts between the recording starts and the user starts talking as well as after the end of speech.
That helps to speed up the training phase because it reduces the size of training data.

\imagefigurelarge{time_signal.jpg}{Illustration of raw speech signal from wav file with sampling frequency of 8kHz\cite{signal_raw}}
%http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

\subsection{MFCC}

Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in speech recognition.
They were introduced by Davis and Mermelstein in the 1980's, and have been state-of-the-art ever since \cite{mfcc_intro}.
%Steven B. Davis and Paul Mermelstein. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. Acoustics, Speech and Signal Processing, IEEE Transactions on, 28(4):357â€“ 366, 8 1980. ISSN 0096-3518. doi: 10.1109/TASSP.1980.1163420.

MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics \cite{mfcc_what}.
%http://recognize-speech.com/feature-extraction/mfcc

\imagefigure{mfcc_diagram.png}{Steps of MFCC \cite{mfcc_steps}}
%https://www.researchgate.net/figure/Block-Diagram-of-MFCC_fig2_281684338

To obtain MFCC features we have to follow operation steps as shown on Figure \ref{pic:mfcc_diagram.png}:

\begin{itemize}
	\item \textit{\textbf{Pre-Emphasis}} - This step applies filter on the speech signal to amplify the high frequencies. It balances the frequency spectrum and avoids numerical problems during the Fourier transform operation.
    \[ y(t) = x(t) - {\alpha}x(t-1) \]
    where $x(t)$ is amplitude of signal in time $t$ and $\alpha$ is filter coefficient which typical values are $0.95$, $y(t)$ pre-emphasis speech signal.

	\item \textit{\textbf{Framing}} - The process of segmenting the speech signal into a small frames with the length within the range of 10 to 40 milliseconds.
  Speech is non-stationary signal but we consider all frames behave stationary so they describe a phonemes.
  In SR we process overlapping frames because phonemes can dependent, resulting to smoother changes in values.
  Popular settings are 25 ms for the frame size, 10 ms stride (15 ms overlap) \cite{signal_raw}.
  %http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

  \item \textit{\textbf{Windowing}} - This step applies Hamming window function\cite{windowing} on each speech signal frame. This is common operation for sound signal before applying FFT.
  %http://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf

  \item \textit{\textbf{FFT}} - This step converts all speech frames from time domain into frequency domain using Fast Fourier Transform (FFT) \cite{fft}.

  \item \textit{\textbf{Mel Filter Banks}} - This step applies the mel-filterbank which consists of triangular overlapping windows that are spread over the whole frequency range, outputting mel frequency spectrum.
	It mimics the non-linear human ear perception of sound, these filters are more discriminative at lower frequencies and less discriminative at higher frequencies \cite{mel_filter_banks}.
	%https://arxiv.org/pdf/1712.06086.pdf

  \item \textit{\textbf{Logarithm}} - This step computes the logarithm of the mel frequency spectrum, to mimic the human perception of loudness because perceive loudness on a logarithmic scale.

  \item \textit{\textbf{DCT}} - This step converts mel spectrum into time domain using Discrete Cosine Transform (DCT)\cite{dcs}, resulting to MFCC vectors.

\end{itemize}
We have just given a theoretical overview how MFCC is calculated, for more detailed explanation consider reading \cite{mfcc_what}.
%http://recognize-speech.com/feature-extraction/mfcc
On Figure \ref{pic:mfcc_raw.jpg} is vector of MFCCs calculated from speech signal Figure \ref{pic:time_signal.jpg} where number of cepstral coefficents is set to $13$.
We have extracted the features of speech signal and vectors of MFCCs can be fed to recognizer.

\imagefigurelarge{mfcc_raw.jpg}{Vector of Mel Frequency Cepstral Coefficents through time.}


\section{Traditional Speech Recognizers}


Historically, most speech recognition systems have been based on a set of statistical models representing the various sounds of the language to be recognized.
We can define a problem of speech recognition as maximizing a probability of the word sequence given some utterance.
\[ W^{*} = \argmax_W P(W|X) \]
where $X$ are acoustic vectors and transcribed $W^{*}$ word sequence.
However, calculating directly $W^{*}$ is a very difficult task. We may simplify it by using Bayes rule resulting to equivalent equation
\[ W^{*} = \argmax_W {P(X|W)P(W)} \]
where the likelihood $P(X|W)$ is called the acoustic model and the prior $P(W)$ is the language model.
\imagefigurelarge{traditional_sr.png}{Diagram of traditional speech recognizer}
In traditional speech recognizers we don't form words directly but we concatenating phonemes which are basic building block of words and they are defined by pronunciation model.
As shown on Figure \ref{pic:traditional_sr.png}, the decoder block works with language, acoustic and pronunciation model.
The language model has a word sequences probabilities, while the acoustic model is generated by Hidden Markov Model (HMM) which is a tool for representing probability distribution over sequences of phonemes using pronunciation model\cite{hmm_sr}.
%http://recognize-speech.com/acoustic-model/hidden-markov-models
%Gaussian mixture model (GMM) is responsible for emitting the probabilities of a phones given the acoustic vectors to an HMM model, which is responsible for modeling the possible transitions at each time step and allows the occurrence of multiple pronunciations (provided by the pronunciation model P(L|W )).

In this thesis we just provide a basic overview how traditional speech recognizers, our main focus is on end-to-end recognizers.


\section{End-to-End Speech Recognizers}

Recent advances in algorithms and computer hardware have made it possible to train neural networks in an end-to-end fashion for tasks that previously required significant human expertise.
All of the state-of-the-art speech recognizers were HMM-based, they required pronunciation, acoustic and language model which were hand-engineered and trained separately.
Not only speech recognizers based on neural networks networks require less human effort than traditional approaches, they generally deliver superior performance \cite{end-to-end}.
%http://proceedings.mlr.press/v32/graves14.pdf
Training independent components is complex and suboptimal compared to training all components as one.
Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages\cite{end-to-end-mandarin}.
%http://proceedings.mlr.press/v48/amodei16.pdf
End-to-end speech recognizers simplifies the training and deployment process altogether.

\subsection{Connectionist Temporal Classification}

Connectionist Temporal Classification (CTC) were introduced in \ref{ctc} a it is the best fit for end-to-end speech recognizer.
Diagram on Figure \ref{pic:ctc_speech.png} shows overview of the model architecture.
Speech can be interpreted as time sequence, thus RNN with LSTM cells will be used since they are they are designed to deal with sequenctional data through time.
The CTC make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown.

\imagefigure{ctc_speech.png}{End-to-end speech recognizer diagram using CTC \cite{ctc_pic}}
%https://www.silversparro.com/single-post/2018/01/26/Making-CTCs-Deep-Learning-Function-work-for-Speech-Recognition

For example, this type of speech recognition model is used in Google Voice Search on Android and iOS \cite{google_search}.
%https://ai.googleblog.com/2015/09/google-voice-search-faster-and-more.html


\subsection{Listen, Attend and Spell}

Listen, Attend and Spell (LAS) is current state-of-the-art end-to-end speech recognizer \cite{las}.
%http://storage.googleapis.com/pub-tools-public-publication-data/pdf/74a8df45b9583e193e6cf8e156dfba9b73c33a0c.pdf

It is based on \textit{Attention Mechanism} \cite{attetion_mech}, created from \textit{Encoder} that reads reads and encodes a source sentence into a fixed-length vector and a \textit{Decoder} that outputs a translation from the encoded vector.
%http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
Attention Mechanisms are now considered one of the most exciting advancements in the field of AI.

LAS is consisted from an encoder recurrent neural network (RNN), which is named the \textit{listener}, and a decoder RNN, which is named the \textit{speller}.
The listener is a pyramidal RNN that converts low level speech signals into higher level features.
The speller is an RNN that converts these higher level features into output utterances by specifying a probability distribution over sequences of characters using the attention mechanism \cite{las_info}.
Both the listener and speller are trained jointly which is the motivation of end-to-end speech recognizers.
%https://arxiv.org/abs/1508.01211

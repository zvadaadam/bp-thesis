\chapter{Speech Recognition}

Speech recognition is the task of converting speech audio to text reprezentation.
It has been attracting researchers for many years with a goal to produce efficient speech recognizer, beacuse it's a very easy and natural human-machine interface tool.

Speech recognition system (ASR) takes audio signal as an input and predicts its transcript.
ASR are normally divided into two important stages as shown on *Figure 1.1*.
The Feature Extractor block generates a sequence of feature vectors which are then feeded to the recognizer block genereting the correct output word.

\imagefigurelarge{speech_recognizer.png}{Basic building blocks of a Speech Recognizer}
%http://www.informatik.uni-ulm.de/ni/staff/FSchwenker/lit/papers/zegers98speech.pdf

\section{Feature Extraction}

The feature extraction (FE) block used in speech recognition should aim towards reducing the complexity of the problem,
it should derive descriptive features from speech signal to enable a classification of sounds.
It is needed because the raw speech signal contains other information besides the linguistic message which would be counterproductive for recognizer.



\subsection{Preprocessing}

It is adventegous to apply preprocessing to raw speech signal before moving to feature extraction block.
Using some type of preprocessing leads to easier feature extraction and faster training phase.

Usually, speech is recorded with a sampling frequency of 44.1kHz (44,100 readings per second). According to The Shannon Theorem *Reference*,
%A. J. Jerri, “The shannon sampling theorem—its various extensions and applications: A tutorial review,” Proceedings of the IEEE, vol. 65, no. 11, pp. 1565–1596, 1977.
a bandwidth limited signal can be reconstructed if the sampling frequency is more than double the maximum frequency meaning that frequencies up to almost 8kHz* are constituted correctly.
Other part of preprocessing is to remove the parts between the recording starts and the user starts talking as well as after the end of speech.
That helps to speed up the training phase because it reduces the size of training data*Reference*.
%https://pdfs.semanticscholar.org/69da/b8bf2f729ed94f53a2dd5df03799258b34a8.pdf

\imagefigurelarge{time_signal.jpg}{Illustration of raw speech signal from wav file with sampling frequency of 8kHz}
%http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

\subsection{MFCC}

Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in speech recognition.
They were introduced by Davis and Mermelstein in the 1980's, and have been state-of-the-art ever since.
%Steven B. Davis and Paul Mermelstein. Comparison of parametric represen- tations for monosyllabic word recognition in continuously spoken sentences. Acoustics, Speech and Signal Processing, IEEE Transactions on, 28(4):357– 366, 8 1980. ISSN 0096-3518. doi: 10.1109/TASSP.1980.1163420.

MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics.
%http://recognize-speech.com/feature-extraction/mfcc

\imagefigure{mfcc_diagram.png}{Steps of MFCC.}
%https://www.researchgate.net/figure/Block-Diagram-of-MFCC_fig2_281684338

To obtain MFFC features we have to follow operation steps as shown on *Figure*:

\begin{itemize}
	\item \textit{\textbf{Pre-Emphasis}} - This step applies filter on the speech signal to amplify the high frequencies. It balances the frequency spectrum and avoids numerical problems during the Fourier transform operation.
    \[ y(t) = x(t) - {\alpha}x(t-1) \]
    where $x(t)$ is amplitude of signal in time $t$ and $\alpha$ is filter coefficient which typical values are $0.95$, $y(t)$ pre-emphasis speech signal.

	\item \textit{\textbf{Framing}} - The process of segmenting the speech sinal into a small frames with the length within the range of 10 to 40 milliseconds.
  Speech is non-stationary signal but we consider all frames behave stationary so they describe a phonemes.
  In SR we process overlapping frames because phonemes can dependent, resulting to smoother changes in values.
  Popular settings are 25 ms for the frame size, 10 ms stride (15 ms overlap)*Reference*.
  %http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html

  \item \textit{\textbf{Windowing}} - This step applies Hamming window function*Refrence* on each speech signal frame. This is common operation for sound signal before applying FFT.
  *Refernce why* %http://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf

  \item \textit{\textbf{FFT}} - This step converts all speech frames from time domain into frequency domain using Fast Fourier Transform (FFT).*Reference FFT*

  \item \textit{\textbf{Mel Filter Banks}} - This step applies the mel-filterbank which consists of triangular overlapping windows that are spread over the whole frequency range, outputing mel frequency spectrum.
	It mimics the non-linear human ear perception of sound, these filters are more discriminative at lower frequencies and less discriminative at higher frequencies.
	%https://arxiv.org/pdf/1712.06086.pdf

  \item \textit{\textbf{Logarithm}} - This step computes the logarithm of the mel frequency spectrum, to mimic the human perception of loudness because perceive loudness on a logarithmic scale*Reference*.
  %H. Niemann, Klassifikation von Mustern, 2nd ed. Berlin, New York, Tokyo: Springer, 2003.

  \item \textit{\textbf{DCT}} - This step converts mel spectrum into time domain using Discrete Cosine Transform (DCT)*Reference*, resulting to MFCC vectors.

\end{itemize}
We have just given a theoretical overview how MFCC is calculated, for more detailed explenation consider reading *Reference MFCC explanation*.
%http://recognize-speech.com/feature-extraction/mfcc
On *figure mfcc* is vector of MFCCs calculated from speech signal *figure signal* where number of cepstral coefficents is set to $13$.
We have extracted the features of speech signal and vectors of MFCCs can be feeded to recognizer.

\imagefigurelarge{mfcc_raw.jpg}{Vector of Mel Frequency Cepstral Coefficents through time.}



\section{Traditional Speech Recognizers}


Historically, most speech recognition systems have been based on a set of statistical models representing the various sounds of the language to be recognized.
We can define a problem of speech recognition as maximizing a probability of the word sequence given some utterance.
\[ W^{*} = \argmax_W P(W|X) \]
where $X$ are acoustic vectors and transcribed $W^{*}$ word sequence.
However, calculating directly $W^{*}$ is a very difficult task. We may simplify it by using Bayes rule resulting to equivalent equation
\[ W^{*} = \argmax_W {P(X|W)P(W)} \]
where the likelihood $P(X|W)$ is called the acoustic model and the prior $P(W)$ is the language model.
%, and $P(X)$ is the observation. FOOTNOTE?
\imagefigurelarge{traditional_sr.png}{Diagram of traditional speech recognizer}
In traditional speech recognizers we don't form words directly but we concatenating phonemes which are basic building block of words and they are defined by pronunciation model.
As shown on *Figure diagram*, the decoder block works with language, acoustic and pronunciation model.
The language model has a word sequences probabilities, while the acoustic model is generated by Hidden Markov Model (HMM) which is a tool for representing probability distribution over sequences of phonemes using pronunciation model.
%http://recognize-speech.com/acoustic-model/hidden-markov-models
%Gaussian mixture model (GMM) is responsible for emitting the probabilities of a phones given the acoustic vectors to an HMM model, which is responsible for modeling the possible transitions at each time step and allows the occurrence of multiple pronunciations (provided by the pronunciation model P(L|W )).

In this thesis we just give a basic overview how traditional speech recognizers, our main focus is on end-to-end recognizers.


\section{End-to-End Speech Recognizers}

Recent advances in algorithms and computer hardware have made it possible to train neural networks(*section*) in an end-to-end fashion for tasks that previously required significant human expertise.
%http://proceedings.mlr.press/v32/graves14.pdf
All of the state-of-the-art speech recognizers were HMM-based, they required pronunciation, acoustic and language model which were hand-engineered and trained separetly.
Not only speech recognizers based on nerual networks networks require less human effort than traditional approaches, they generally deliver superior performance.
%http://proceedings.mlr.press/v32/graves14.pdf
Training independent components is complex and suboptimal compared to training all components as one.
Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.
%http://proceedings.mlr.press/v48/amodei16.pdf
End-to-end speech recognizers simplifies the training and deployment process altogether.

\subsection{Connectionist Temporal Classification}

Connectionist Temporal Classification (CTC) were introduced in *section* a it is the best fit for end-to-end speech recognizer.
Diagram on Figure \ref{pic:ctc_speech.png} shows overview of the model architeture.
Speech can be interprted as time sequence, thus RNN with LSTM cells will be used since they are they are designed to deal with sequenctional data through time.
The CTC make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown.

\imagefigure{ctc_speech.png}{End-to-end speech recognizer diagram using CTC*Ref*}
%https://www.silversparro.com/single-post/2018/01/26/Making-CTCs-Deep-Learning-Function-work-for-Speech-Recognition

For example, this type of speech recognition model is used in Google Voice Search on Android and iOS*ref*.
%https://ai.googleblog.com/2015/09/google-voice-search-faster-and-more.html


\subsection{Listen, Attend and Spell}

Listen, Attend and Spell (LAS) is current state-of-the-art end-to-end speech recognizer *REF*.
%http://storage.googleapis.com/pub-tools-public-publication-data/pdf/74a8df45b9583e193e6cf8e156dfba9b73c33a0c.pdf

It consists of an encoder recurrent neural network (RNN), which is named the listener, and a decoder RNN, which is named the speller.
The listener is a pyramidal RNN that converts low level speech signals into higher level features.
The speller is an RNN that converts these higher level features into output utterances by specifying a probability distribution over sequences of characters using the attention mechanism.
The listener and the speller are trained jointly.

*TODO*

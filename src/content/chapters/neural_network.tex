\chapter{Neural Network}


Neural networks have a remarkable ability to derive meaning from complicated data. They can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques\cite{introduction_ann}.
Even though they have been around since the 1950s, it is only in the last decade when they started to outperform robust system or even humans in specify tasks.
However, they require a huge amount of training examples and computational power to be trained for preforming a reasonable prediction.
Fortunately, GPUs has seen enormous increase in performance\footnote{\label{note1}GPUs are explicitly designed to handle multiple matrix calculations at the same time. Evaluation and training of artificial neural networks are mostly matrix operations.} and
90\% of the data in the world today has been created in the last two years alone, at 2.5 quintillion bytes of data a day\cite{ibm_data}.
That's why ANN is big topic in Computer Science and in the technology industry and it currently provides the best solutions to many problems such as speech recognition, image recognition, and natural language processing.

\section{Inspiration in Nature}

Artificial neural network (ANN) is heavily inspired by the way how biological neural networks process information in the human brain.
Even though our brain is extremely complex and still not fully understand, we just need to know how information is being transferred.
The basic building block is nerve cell called \textit{neuron}. It receives, processes, and transmits information through electrical and chemical signals\cite{neuron}.
It's estimated that an average human has 86 billion neurons\cite{number_of_neurons}.

\imagefigurelarge{nerve_cell.jpg}{Illustration of nerve cell and communication flow}

As shown on Figure \ref{pic:nerve_cell.jpg}, \textit{dendrites} are extensions of a nerve cell that propagate the electrochemical stimulation received from other neurons to the cell body.
You may think of them as inputs to neuron, whereas neuron's output is called \textit{axon}, a long nerve fiber that conducts electrical impulses away from the cell body.
The end of axon is branched to many axon terminals which can be again connected to other dendrites.
The connection is managed by \textit{synapses} that can permit the passing of electrical signal to cell body.
Once the cell reaches a certain threshold, an action potential will fire, sending the electrical signal down the axon to other connected neurons.

\section{Artificial Neuron}

Artificial neuron is a generic computational unit, basic building block for artificial neural network (ANN).
It's simplified version of the biological counterpart and we are able to map parts of biological neuron with the artificial one.
It takes \textit{n} inputs represented as a vector $x\in\mathbb{R}^n$ which correspond to dendrites.
Generaly artificial neuron produces single output $y\in\mathbb{R}$ as biological neuron where we call it axon.
Each neuron's input $i=1,2,\ldots, n$ has assigned weight (synapse) $w_1, w_2 \ldots w_n$, they refer to the connection strength between neurons.
Weights and same as for synapse are the backbone of learning because in training phases, they keep changing to produce wanted output.
Inside the artificial neuron, input vector with their weights are combined and run through an activation function producing some output $y$.
This process is illustrated in Figure \ref{pic:preceptron.png}.

\imagefigurelarge{preceptron.png}{Illustration of nerve cell and communication flow}

\section{Perceptron}

Perceptron is the simplest ANN with just one neuron and since we covered the basic intuition about artificial neuron we may proceed further and take a look at how output is actually calculated.
The equation for a perceptron can be written as

\begin{equation} \label{eq:1}
 y = f(\sum_{i=1}^N w_i \cdot x_i + b)
\end{equation}

where

\begin{itemize}
	\item $x$ - input vector
	\item $y$ - predicted output
  \item $f$ - activation function
	\item $w$ - weights
	\item $b$ - bias
\end{itemize}

Perceptron is a basically linear classifier, therefore the data has to be linearly separable otherwise we would not be able to make the correct prediction.
Problems such as speech recognition are not definitely linearly separable, however we can solve non-linear decisions for example by introducing another layer of neurons, thus creating \textit{Multilayered Perceptron}.

\subsection{Activation Functions}

We have stated that biological neuron fires electrical signal to other connected neurons whenever it reaches a certain threshold of incoming electrical impulses.
Activation function is based on that concept and inside an artificial neuron it is used for calculating output signal via equation \ref{eq:1}.
It introduces non-linear properties to our ANN and without an activation function would be just a regular linear regression model.
Nowadays many different activation function are being used and their performance varies from model to model. \newline
\newline
List of some activation function:

\begin{itemize}
	\item \textit{Sigmoid} \[ \sigma(x)=\frac{1}{1+e^{-x}} \]
	\item \textit{Hyperbolic Tangent} \[ \tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})} \]
  \item \textit{ReLU}  \[ f(x) = \begin{cases} 0 & \text{for } x < 0 \\ x & \text{for } x \ge 0\end{cases} \]
  \item \textit{Softmax} \[ f_i(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^J e^{x_j}}, \quad i = 1, 2 \ldots J \]
    where $i$ is number of output
\end{itemize}

\subsection{Bias}

We can think of bias as a value stored inside neuron and being used to calculate it's output.
The bias value allows the activation function to be shifted to the left or right, to better fit the data.


\section{Topology of Artificial Neuron Network}

\imagefigure{ann.png}{Basic topology of fully connected artificial neuron network with input vector of size 3, output vector of size 2 and two hidden layers.}

Basic ANN as feedforward model is a directed graph with nodes as neurons and edges with weights representing connection to other neurons.
ANN can be divided to three important layers as shown in Figure \ref{pic:ann.png}.
Yellow nodes is an input layer which takes input data, dimension of input vector has to correspond to number of input nodes.
Hidden layer as the green nodes is most important to ANN and that is where the training and evaluation happens.
Number of hidden layers and neurons needs to be in a good ratio between its size and its effectiveness.
Output layer produces output vector as the prediction for given input.


\subsection{Network Evaluation}

ANN are sometimes called feedforward neural network.
The reason behind is that the input is fed into the neuron and then forward to another layer, thus ANN are evaluated layer by layer.
All neurons calculates the output using similar formula as Perceptron \ref{eq:1}.

\section{Training}

The greatest trait of ANN is ability to learn from given data and then make the best approximate prediction.
The aim of the learning process is to find the most optimal values for network's weights and biases while minimizing error on predicated values.
For ANN to learn we have to introduce training data consisted of input vector which will be fed to the network and desired output value (label) for calculating our loss.
This approach is called supervised learning\footnote{\label{note1}ANN can be also trained using unsupervised learning.}.
\newline

\subsection{Loss Function}

Loss function compares the prediction from ANN with the desired output and returns the error of the prediction.
During a training ANN, the goal is to minimize given loss function.
The most common and most intuitive loss function is Mean squared Error (MSE),
\[ \operatorname{MSE}(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2. \]


\subsection{Backpropagation}

Backpropagation algorithm is responsible for the ability to learn from given training data.
It is an iterative algorithm which for each training data %(input vector, label)
from given training dataset backpropagates the error and adjust the weights and biases accordingly to get desired output.

\subsubsection{Optimization}

Backpropagation requires optimizer to minimize the error on the training data.
We will describe backpropagation with using \textit{gradient descent} as the most common optimization algorithm.
\newline
Weights and biases are updated using formula,
\begin{equation} \label{eq:2}
	W_{jk}^l := W_{jk}^l - \alpha\pd{E}{W_{jk}^l}
	b_j^l := b_j^l - \alpha\pd{E}{b_j^l}
\end{equation}
where $\boldsymbol{W_{jk}^l}$ is weight with connection between unit $j$ in layer $l$ and unit $i$ in layer $l+1$, $\boldsymbol{b_j^l}$ is bias associated with unit
$i$ in layer $l+1$, $\boldsymbol{\alpha}$ is a learning rate \cite{learning_rate}, and $\boldsymbol{\pd{E}{W_{jk}^l}}$ or $\boldsymbol{\pd{E}{b_j^l}}$ can be interpreted as minimizing loss function with respect to given weight and bias respectively.
\par
By applying a chain rule twice on the partial derivative of the loss function with respect to a weight, we get
\begin{equation} \label{eq:3}
  \pd{E}{W_{jk}^l} = \pd{E}{a_j^l}\pd{a_j^l}{z_j^l}\pd{z_j^l}{W_{jk}^l}
\end{equation}
where $\boldsymbol{z_j^l}$ is a sum of weighted inputs to unit $j$ in layer $l$
\begin{equation} \label{eq:4}
	z_j^l =  b_j^l + \sum_{k=1}^K {w_{jk}^l}{a_k^{l-1}}
\end{equation}
and $\boldsymbol{a_j^l}$ is an output of node $j$ in layer $l$
\begin{equation} \label{eq:5}
	a_j^l =  f(z_j^l).
\end{equation}
Let's calculate the last two products of equation \ref{eq:3}:
\begin{equation} \label{eq:6}
	\pd{a_j^l}{z_j^l} = f'(z_j^l)
	\pd{z_j^l}{W_{jk}^l} = \pd{W_{jk}^{l}a_k^{l}}{W_jk^l} = a_k^{l-1}
\end{equation}
We introduce a new varibale $\boldsymbol{\delta_j^l}$ which represents the error in unit $j$ in layer $l$ and helps us to better understand and calculate real interested value of \pd{E}{W_{jk}^l} and \pd{E}{b_j^l}.
\begin{equation} \label{eq:7}
	\delta_j^l = \pd{E}{z_j^l}
\end{equation}
We will simplify the error equation on neuron $j$ in output layer $L$ as
\begin{equation} \label{eq:8}
  \delta_j^L = \pd{E}{z_j^L} = \pd{E}{a_j^L}\pd{a_j^L}{z_j^L} = \pd{E}{a_j^L}f'(z_j^L)
\end{equation}
Now we have enough information to reformulate equation \ref{eq:3} for output layer to
\begin{equation} \label{eq:9}
	\pd{E}{W_{jk}^l} = \delta_k^{L}a_j^L.
\end{equation}
However, to be able to update weights inside the hidden layers, we have to redefine the calculation of $\delta_j^l$.
We know that the error produced by an output neuron is just influencing the output value but inside a hidden layer the produced error propagates to all following layers.
Therefore we have calculate the $\delta_j^l$ where layer $l$ is inside a hidden layer and take into account all $\delta^{l+1}$ from following layer $l+1$.
\begin{equation} \label{eq:10}
	\delta_j^l =  \pd{E}{z_j^l} = \sum_i\pd{E}{z_i^{l+1}}\pd{z_i^{l+1}}{z_j^{l}} = \sum_i\pd{E}{z_i^{l+1}}\pd{z_i^{l+1}}{a_j^{l}}\pd{a_j^l}{z_j^l}
	= \sum_i\delta_i^{l+1}W_{ij}^{l+1}f'(z_j^l)
\end{equation}
where the sum index $i$ iterates over all neurons in layer $l+1$ and Notice that we have substituted $\pd{E}{z_i^{l+1}}$ with $\delta_i^{l+1}$ which is calculated from previous iteration\ref{gradient_equation}.
\newline
Finally, we may calculate all weights adjustments through the whole network as
\begin{equation} \label{eq:11}
	W_{jk}^l := W_{jk}^l - \alpha\delta_k^{l}a_j^l
\end{equation}
where
\begin{equation} \label{eq:12}
	\delta_k^{l} = \pd{E}{a_j^L}f'(z_j^L), \quad l = L
\end{equation}
or
\begin{equation} \label{eq:13}
	\delta_k^{l} = \sum_i\delta_i^{l+1}W_{ij}^{l+1}f'(z_j^l), \quad l = 2,\ldots, L-1.
\end{equation}

We won't be exampling the equation for biases adjustments because it follows a similar process shown above with just little changes, resulting to equation
\begin{equation} \label{eq:14}
	b_{j}^l := b_{j}^l - \alpha\delta_l^{j}
\end{equation}

\subsubsection{Backpropagation Algortihm}

Backpropagation algortihm in pseudocode:

\begin{algorithm}
\caption{Backpropagation}\label{euclid}
\begin{algorithmic}[1]
\State Initialize network weights and biases
\For{\textbf{each} training data from training dataset}
	\State Forward pass and calculate network prediction for given training input
	\State Calculate error $\delta^L$ for output layer
	\State Calculate errors $\delta^l$ for hidden layers
	\State Update weights and biases using precalculated $\delta^l$
\EndFor
\end{algorithmic}
\end{algorithm}

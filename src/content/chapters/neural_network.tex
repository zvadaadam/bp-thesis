\chapter{Neural Network}




// TODO: Why ANN are awesome :)

While artificial neural networks (ANN) have been around since the 1940s, it is only in the last several decades where they have become a major part of artificial intelligence.


An artificial neural network (ANN) is mathematical model, heavily inspired by the way how biological neural networks process information in the human brain.
In general we can view ANNs as function of f: X -> Y where X is the input to neural network and Y is aproximation of our target function.
In general we can view ANNs as function that maps

It's achived by gradule from

What do the do?
How do they do it?
Supervised learning

How its structred?

While neural networks have been around since the 1940s, it is only in the last several decades where they have become a major part of artificial intelligence.


In general we can view ANNs as function of f: X -> Y

high level of versatility

ANNs aim to reach high level of versatility as our brain does.

In general ANNs are able to reach high level of versatility as our brain because they are aproximating

Inspired by biological nervous systems, artificial neural networks (ANNs) aim at reaching their versatility through learning

While neural networks have been around since the 1940s, it is only in the last several decades where they have become a major part of artificial intelligence.


\section{Inspiration in Nature}

Artificial neural network (ANN) is heavily inspired by the way how biological neural networks process information in the human brain.
Even though our brain is extremely complex and still not fully understand, we just need to know how information is being transferred.
The basic building block is nerve cell called \textit{neuron}. It receives, processes, and transmits information through electrical and chemical signals.
It's estimated that an average human has 86 billion neurons *.

\imagefigurelarge{nerve_cell.jpg}{Illustration of nerve cell and communication flow}

\textit{Dendrites} are extensions of a nerve cell that propagate the electrochemical stimulation received from other neurons to the cell body.
You may think of them as inputs to neuron, whereas neuron's output is called \textit{axon}, a long nerve fiber that conducts electrical impulses away from the cell body.
The end of axon is branched to many axon terminals which can be again connected to other dendrites.
The connection is managed by \textit{synapses} that can permit the passing of electrical signal to cell body.
Once the cell reaches a certain threshold, an action potential will fire, sending the electrical signal down the axon to other connected neurons.

\section{Artificial Neuron}

Artificial neuron is a generic comuptational unit, basic building block for artificial neural network (ANN).
It's simplified version of the biological counterpart and we are able to map parts of biological neuron with the artificial one.
It takes \textit{n} inputs represented as a vector $x\in\mathbb{R}^n$ which correspond to dendrites.
Generaly artificial neuron produces single output $y\in\mathbb{R}$ as biological neuron where we call it axon.
Each neuron's input $i=1,2,\ldots, n$ has assigned weight (synapse) $w_1, w_2 \ldots w_n$, they refer to the connection strength between neurons.
Weights and same as for synapse are the backbone of learning beacuse in training phases, they keep changing to produce wanted output.(*In this chapter, we will elaborate further.)
Inside the artificial neuron, input vector with their weights are combined and run through an activation function producing some output $y$.
This process is illustrated in $LINK_PRECEPRTON$.

\imagefigurelarge{preceptron.png}{Illustration of nerve cell and communication flow}

\section{Perceptron}

Perceptron is the simplest ANN with just one neuron and since we covered the basic intuition about artificial neuron we may proceed further and take a look at how output is actually calculated.
The equation for a perceptron can be written as

\[ y = f(\sum_{i=1}^N w_i \cdot x_i + b) \]

where

\begin{itemize}
	\item $x$ - input vector
	\item $y$ - predicted output
  \item $f$ - activation function
	\item $w$ - weights
	\item $b$ - bias
\end{itemize}

Perceptron is a basically linear classifier, therefore the data has to be linearly separable otherwise we would not be able to make the correct prediction.
Problems such as speech recognition are not definitely linearly separable, however we can solve non-linear desicions for example by introducing another layer of neurons, thus creating \textit{Multilayered Perceptron}.

\subsection{Activation Functions}

We have stated that biological neuron fires electrical signal to other connected neurons whenever it reaches a certain threshold of incoming electrical impulses.
Activation function is based on that concept and inside an artificial neuron it is used for calculating output signal via equation*.
It introduces non-linear properties to our ANN and without an activation function would be just a regular linear regression model.
Nowdays many diffrent activation function are being used and their performance veries from model to model. \newline
\newline
List of some activation function:

\begin{itemize}
	\item \textit{Sigmoid} \[ \sigma(x)=\frac{1}{1+e^{-x}} \]
	\item \textit{Hyperbolic Tangent} \[ \tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})} \]
  \item \textit{ReLU}  \[ f(x) = \begin{cases} 0 & \text{for } x < 0 \\ x & \text{for } x \ge 0\end{cases} \]
  \item \textit{Softmax} \[ f_i(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^J e^{x_j}}, \quad i = 1, 2 \ldots J \]
    where $i$ is number of output
\end{itemize}

* TODO: Activation Functions features... diferentable and non-linear*

\subsection{Bias}

We can think of bias as a value stored inside neuron and being used to calculate it's output.
The bias value allows the activation function to be shifted to the left or right, to better fit the data.


\section{Topology of Artificial Neuron Network}

\imagefigure{ann.png}{Basic topology of fully connected artificial neuron network with input vector of size 3, output vector of size 2 and two hidden layers.}

Basic ANN as feedforward model is a directed graph with nodes as neurons and edges with weights representing connection to other neurons.
ANN can be divided to three important layers as shown in Fig*.
Yellow nodes is an input layer which takes input data, dimension of input vector has to correspond to number of input nodes.
Hidden layer as the green nodes is most important to ANN and that is where the training and evaluation happens.
Number of hidden layers and neurons needs to be in a good ratio between its size and its effectiveness.
Output layer produces output vector as the prediction for given input.


\section{Network Evaluation}

Evaluation of ANN is done layer by layer.


\section{Training}

The greatest trait of ANN is ability to learn from given data and then make the best aproximate prediction.
The aim of the learning process is to find the most optimal values for network's weights and biases while minimizing error on predicated values.
For ANN to learn we have to introduce training data consisted of input vector which will be feeded to the network and desired output value (\texit{label}) for calculating our loss.
This aproach is called supervised learning\footnote{\label{note1}ANN can be also trained using unsupervised learning.}.
\newline

\subsection{Loss Function}

Loss function compares the prediction from ANN with the desired output and returns the error of the prediction.
During a training ANN, the goal is to minimize given loss fucntion.
The most common and most intuitive loss function is Mean squared Error (MSE),
\[ \operatorname{MSE}(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2. \]


\subsection{Backpropagation}

Backpropagation algorithm is resposible for the ability to learn from given training data.
It is an iterative algortihm which for each training data %(input vector, label)
from given training dataset backpropagates the error and adjust the weights and biases accordingly to get desired output.

\subsubsection{Optimization}

Backpropagation requires optimizer to minimize the error on the training data.
We will describe backpropagation with using \textit{gradient descent} as the most common optimization algorithm.
\newline
Weights and baises are updated using formula,
\[  W_{jk}^l := W_{jk}^l - \alpha\pd{E}{W_{jk}^l} \]
\[ b_j^l := b_j^l - \alpha\pd{E}{b_j^l} \]
where $\boldsymbol{W_{jk}^l}$ is weight with connection between unit $j$ in layer $l$ and unit $i$ in layer $l+1$, $\boldsymbol{b_j^l}$ is bias associated with unit
$i$ in layer $l+1$, $\boldsymbol{\alpha}$ is a learning rate *Reference*, and $\boldsymbol{\pd{E}{W_{jk}^l}}$ or $\boldsymbol{\pd{E}{b_j^l}}$ can be interpreted as minimizing loss function with respect to given weight and bias respectively.
\par
By applying a chain rule twice on the partial derivative of the loss function with respect to a weight, we get
\[  \pd{E}{W_{jk}^l} = \pd{E}{a_j^l}\pd{a_j^l}{z_j^l}\pd{z_j^l}{W_{jk}^l} \]
where $\boldsymbol{z_j^l}$ is a sum of weighted inputs to unit $j$ in layer $l$
\[  z_j^l =  b_j^l + \sum_{k=1}^K {w_{jk}^l}{a_k^{l-1}}  \]
and $\boldsymbol{a_j^l}$ is an output of node $j$ in layer $l$
\[  a_j^l =  f(z_j^l).  \]
Let's calculate the last two products of equation *Reference*:
\[  \pd{a_j^l}{z_j^l} = f'(z_j^l)  \]
\[  \pd{z_j^l}{W_{jk}^l} = \pd{W_{jk}^{l}a_k^{l}}{W_jk^l} = a_k^{l-1}  \]
We introduce a new varibale $\boldsymbol{\delta_j^l}$ which represents the error in unit $j$ in layer $l$ and helps us to better understand and calculate real interested value of \pd{E}{W_{jk}^l} and \pd{E}{b_j^l}.
\[  \delta_j^l = \pd{E}{z_j^l} \]
We will simplify the error equation on neuron $j$ in output layer $L$ as
\[  \delta_j^L = \pd{E}{z_j^L} = \pd{E}{a_j^L}\pd{a_j^L}{z_j^L} = \pd{E}{a_j^L}f'(z_j^L)\]
Now we have enough information to reformulate equation *reference* for output layer to
\[ \pd{E}{W_{jk}^l} = \delta_k^{L}a_j^L. \]
However, to be able to update weights inside the hidden layers, we have to redefine the calculation of $\delta_j^l$.
We know that the error produced by an output neuron is just influencing the output value but inside a hidden layer the produced error propagates to all following layers.
Therefore we have calculate the $\delta_j^l$ where layer $l$ is inside a hidden layer and take into account all $\delta^{l+1}$ from following layer $l+1$.
\[  \delta_j^l =  \pd{E}{z_j^l} = \sum_i\pd{E}{z_i^{l+1}}\pd{z_i^{l+1}}{z_j^{l}} = \sum_i\pd{E}{z_i^{l+1}}\pd{z_i^{l+1}}{a_j^{l}}\pd{a_j^l}{z_j^l} \]
\[  = \sum_i\delta_i^{l+1}W_{ij}^{l+1}f'(z_j^l) \]
where the sum index $i$ iterates over all neurons in layer $l+1$ and Notice that we have substituted $\pd{E}{z_i^{l+1}}$ with $\delta_i^{l+1}$ which is calculated from previous iteration.
\newline
Finally, we may calculate all weights adjustments through the whole network as
\[  W_{jk}^l := W_{jk}^l - \alpha\delta_k^{l}a_j^l \]
where
\[ \delta_k^{l} = \pd{E}{a_j^L}f'(z_j^L), \quad l = L \]
and
\[ \delta_k^{l} = \sum_i\delta_i^{l+1}W_{ij}^{l+1}f'(z_j^l), \quad l = 2,\ldots, L-1. \]

We won't be exampling the equation for baises adjustments because it follows a similar process shown above with just little changes, resulting to equation
\[  b_{j}^l := b_{j}^l - \alpha\delta_l^{j} \]

\subsubsection{Backpropagation Algortihm}

\clearpage

Using our predefined loss function *ReferenceEquation* we will simplify the error on considered neuron,
For example purposes, let us assume that we will use \textit{quadratic loss function},
\[  E =  \frac{1}{2}\sum_j(y_j - a_j^l).  \]

\begin{algorithm}
\caption{Backpropagation}\label{euclid}
\begin{algorithmic}[1]
\State Network evaluation for given input, computing $z^l$ and $a^l$ for each layer
\For{\textbf{each} node $i$ in output layer $L$}
  \State $\delta_i^L = \pd{E}{a_i^L}f'(z_i^L)$
\EndFor
\For{\textbf{each} layer $l = L-1, L-2, \ldots 2$}
  \For{\textbf{each} node $j$ in layer $l$}
    \State $\delta_j^l = \pd{E}{z_j}$
  \EndFor
\EndFor
\State $\pd{E}{w_{jk}^l} = a_k^{l-1}\delta_j^l$
\State $\pd{E}{b_j^l} = \delta_j^l$
\State $w_{jk}^l := w_jk^l - \alpha\pd{E}{w_{jk}^l}$
\State $b_j^l := b_j^l - \alpha\pd{E}{b_j^l}$
\end{algorithmic}
\end{algorithm}
